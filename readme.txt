SoC 2025 - Sentiment Analysis

Mentee - Yashneil Rawat

Meentor - Ankush Kumar Mandal

The project aims to create a Sentiment Analyzer which will process on text to predict the emotion embedded in it, using Natural Language Processing.
Following topics were covered to form a basic understanding required for the project :

1 : Python Modules Fundamentals

    - Familiarize with Python libraries frequently used in data science and NLP.
    - Understand basic syntax and functionality of each module.
    - Set up the project environment (Jupyter Notebook or Google Colab).

    Topics Covered:
        a. Python Revisit
        b. Numpy
        c. Pandas
        d. Matplotlib
        e. Seaborn
        f. Scikit-learn

2 : Machine Learning Fundamentals

    - Understood the basics of machine learning (focus on supervised learning).
    - Learnt common classification algorithms: Logistic Regression, Naive Bayes, SVM, Decision Tree and Random Forest.
    - Understood key concepts: training vs. testing, overfitting, accuracy vs. precision.
    
    Topics Covered :

        a. Supervised and Unsupervised Learning
        b. Linear Regression
        c. Multiple Linear Regression
        d. Regression v/s Classification
        e. Logistic Regression
        f. Decision Tree
        g. SVC
        h. Random Forest Classifier
        i. K-Nearest Neighbour(KNN)
        j. Naive-Bayes

3: Natural Language Processing Basics

    - Understood what Natural Language Processing is and its importance.
    - Got hands-on with basic NLP preprocessing techniques.
    - Learnt to process raw text into a clean, tokenized, and analyzable format.

    Topics Covered :

        a. Regular Expressions (Regex)
        b. Tokenisation
        c. Stopword Removal
        d. Stemming and Lemmatization
        e. POS Tagging
        f. Named Entity Recognition (NER)

4: Transformer

    - Capture context more effectively than traditional models.
    - Enable parallel processing, making training faster.
    - Power pretrained models like:
        a. BERT – Bidirectional Encoder Representations from Transformers (used for classification, QA, etc.)
        b. GPT – Generative Pretrained Transformer (used for generation, summarization, etc.)